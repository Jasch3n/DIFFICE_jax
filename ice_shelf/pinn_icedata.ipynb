{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPyK3R7q7FTXE4uR8b6so4t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YaoGroup/DIFFICE_jax/blob/main/ice_shelf/pinn_icedata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Viscosity inversion for real ice shelves via PINNs"
      ],
      "metadata": {
        "id": "cA8RPIy4vSe5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fast running the code requires **GPU hardware** to accelerate.\n",
        "\n",
        "To choose GPU hardware, click the \"Edit\" in the toolbar above and select \"Notebook settings\".\n",
        "\n",
        "In the pop-up window, under the \"Hardware accelerator\", select \"T4 GPU\""
      ],
      "metadata": {
        "id": "tZQ4X2lgfY0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To ensure the code to run correctly in the colab, install the specific version of JAX below.\n",
        "\n",
        "(Note that only using the GPU version of JAX, combined with the selection of GPU hardware as mentioned above, can eventually accelerate the training.)"
      ],
      "metadata": {
        "id": "jlnGqpgzgf_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install specific version (0.4.23) of JAX and Jaxlib\n",
        "!pip install --upgrade jax==0.4.23 jaxlib==0.4.23+cuda12.cudnn89 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "\n",
        "# Verify the installed version\n",
        "import jax\n",
        "print(jax.__version__)"
      ],
      "metadata": {
        "id": "ei2r5MmucPiG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a649827d-6272-4639-b876-0405190d4cb6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
            "Collecting jax==0.4.23\n",
            "  Downloading jax-0.4.23-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jaxlib==0.4.23+cuda12.cudnn89\n",
            "  Downloading https://storage.googleapis.com/jax-releases/cuda12/jaxlib-0.4.23%2Bcuda12.cudnn89-cp310-cp310-manylinux2014_x86_64.whl (131.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.8/131.8 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.23) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.23) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax==0.4.23) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.23) (1.11.4)\n",
            "Installing collected packages: jaxlib, jax\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.26+cuda12.cudnn89\n",
            "    Uninstalling jaxlib-0.4.26+cuda12.cudnn89:\n",
            "      Successfully uninstalled jaxlib-0.4.26+cuda12.cudnn89\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.26\n",
            "    Uninstalling jax-0.4.26:\n",
            "      Successfully uninstalled jax-0.4.26\n",
            "Successfully installed jax-0.4.23 jaxlib-0.4.23+cuda12.cudnn89\n",
            "0.4.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copying required python functions from the [DIFFICE_jax](https://github.com/YaoGroup/DIFFICE_jax) GitHub repository"
      ],
      "metadata": {
        "id": "7gbfuwvAhaiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/YaoGroup/DIFFICE_jax"
      ],
      "metadata": {
        "id": "eHc_W9sTUBF6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d5b1a1e-cbc7-481d-8501-87055a0031f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DIFFICE_jax'...\n",
            "remote: Enumerating objects: 724, done.\u001b[K\n",
            "remote: Counting objects: 100% (444/444), done.\u001b[K\n",
            "remote: Compressing objects: 100% (265/265), done.\u001b[K\n",
            "remote: Total 724 (delta 339), reused 249 (delta 178), pack-reused 280\u001b[K\n",
            "Receiving objects: 100% (724/724), 25.82 MiB | 11.23 MiB/s, done.\n",
            "Resolving deltas: 100% (430/430), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the required JAX library and function file from GitHub repository"
      ],
      "metadata": {
        "id": "9cv4dMKPhmLE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZSe-qjr3Ju2N"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from jax import random\n",
        "from jax.tree_util import tree_map\n",
        "from scipy.io import loadmat\n",
        "import time\n",
        "\n",
        "from DIFFICE_jax.data.sampling import data_sample_create\n",
        "from DIFFICE_jax.equation.ssa_eqn_iso import vectgrad, gov_eqn, front_eqn\n",
        "from DIFFICE_jax.model.initialization import init_MLP\n",
        "from DIFFICE_jax.model.networks import solu_create\n",
        "from DIFFICE_jax.model.loss import loss_create\n",
        "from DIFFICE_jax.model.prediction import predict\n",
        "from DIFFICE_jax.optimizer.optimizer import adam_optimizer, lbfgs_optimizer\n",
        "from DIFFICE_jax.ice_shelf.load_icedata import iceshelf_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting hyperparameters\n",
        "\n",
        "hyper-parameters used for the training. Users are free to modify their value and check their influence on the training results\n"
      ],
      "metadata": {
        "id": "NIvVtcvew_nY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select a random seed\n",
        "seed = 2134\n",
        "key = random.PRNGKey(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# create the subkeys\n",
        "keys = random.split(key, 4)\n",
        "\n",
        "# select the size of neural network\n",
        "n_hl = 6  # number of hidden layers\n",
        "n_unit = 30  # number of units in each hidden layer\n",
        "\n",
        "# select the weight for the equation and boundary conditions\n",
        "lw = [0.05, 0.1]\n",
        "\n",
        "# number of points sampled for each variable from the synthetic data\n",
        "n_smp = 6000\n",
        "# number of collocation points to evaluate the equation residue\n",
        "n_col = 6000\n",
        "# number of collocation points to evalute boundary conditions\n",
        "n_cbd = 600\n",
        "# group all the points\n",
        "n_pt = jnp.array([n_smp, n_col, n_cbd], dtype='int32')\n",
        "\n",
        "# double all the points for L-BFGS training (fixed points over iterations)\n",
        "n_pt2 = n_pt * 2\n",
        "\n",
        "# select the ice shelf for the training\n",
        "shelfname = 'Amery'\n",
        "\n",
        "# create the dataset filename\n",
        "DataFile = 'Data_' + shelfname + '.mat'\n",
        "# re-organized the synthetic data for the PINN code\n",
        "data_all = iceshelf_data(DataFile, step=1)\n",
        "# extract the scale information for each variable\n",
        "scale = data_all[4][0:2]\n"
      ],
      "metadata": {
        "id": "wFBWogcjXcEe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing the model and the loss function"
      ],
      "metadata": {
        "id": "0X6CNFhfxOL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the weights and biases of the network\n",
        "trained_params = init_MLP(keys[0], n_hl, n_unit)\n",
        "\n",
        "# create the solution function\n",
        "pred_u = solu_create()\n",
        "\n",
        "# create the data sampling function for Adam training\n",
        "dataf = data_sample_create(data_all, n_pt)\n",
        "keys_adam = random.split(keys[1], 5)\n",
        "# generate the data\n",
        "data = dataf(keys_adam[0])\n",
        "\n",
        "# create the data sampling function for L-BFGS training\n",
        "dataf_l = data_sample_create(data_all, n_pt2)\n",
        "key_lbfgs = random.split(keys[2], 5)\n",
        "\n",
        "\n",
        "# group the gov. eqn and bdry cond.\n",
        "eqn_all = (gov_eqn, front_eqn)\n",
        "# create the loss function\n",
        "NN_loss = loss_create(pred_u, eqn_all, scale, lw)\n",
        "# calculate the initial loss and set it as the reference value for loss\n",
        "NN_loss.lref = NN_loss(trained_params, data)[0]\n"
      ],
      "metadata": {
        "id": "fzpbvcDDXunl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training using ADAM optimizer\n",
        "\n",
        "Require at least 10000 iterations of Adam,\n",
        "plus 10000 iterations of L-BFGS later\n",
        "for a relatively accurate trained model\n"
      ],
      "metadata": {
        "id": "6LBYqgjFAuPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the learning rate for Adam\n",
        "lr = 1e-3\n",
        "# set the training iteration\n",
        "epoch1 = 10000\n",
        "\n",
        "# training with Adam\n",
        "trained_params, loss1 = adam_optimizer(keys_adam[1], NN_loss, trained_params, dataf, epoch1, lr=lr)\n"
      ],
      "metadata": {
        "id": "mNVBZd_OYDAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b5a7dca-b0a2-4640-d2c2-fd2f49707241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Step: 20 | Loss: 1.7636e-01 | Loss_d: 2.9135e-01 | Loss_e: 2.0864e-01 | Loss_b: 1.8249e-02\n",
            "Step: 40 | Loss: 1.1493e-01 | Loss_d: 1.7732e-01 | Loss_e: 3.9534e-01 | Loss_b: 7.5765e-03\n",
            "Step: 60 | Loss: 9.7267e-02 | Loss_d: 1.6523e-01 | Loss_e: 3.9759e-02 | Loss_b: 2.2117e-03\n",
            "Step: 80 | Loss: 8.9029e-02 | Loss_d: 1.5177e-01 | Loss_e: 2.5959e-02 | Loss_b: 1.9754e-03\n",
            "Step: 100 | Loss: 7.8914e-02 | Loss_d: 1.3462e-01 | Loss_e: 2.1024e-02 | Loss_b: 1.7285e-03\n",
            "Step: 120 | Loss: 7.6020e-02 | Loss_d: 1.2896e-01 | Loss_e: 3.5013e-02 | Loss_b: 1.5663e-03\n",
            "Step: 140 | Loss: 7.1118e-02 | Loss_d: 1.1897e-01 | Loss_e: 6.5185e-02 | Loss_b: 1.9583e-03\n",
            "Step: 160 | Loss: 6.7068e-02 | Loss_d: 1.1122e-01 | Loss_e: 7.8701e-02 | Loss_b: 3.0145e-03\n",
            "Step: 180 | Loss: 6.1565e-02 | Loss_d: 1.0081e-01 | Loss_e: 9.5210e-02 | Loss_b: 4.1576e-03\n",
            "Step: 200 | Loss: 5.5484e-02 | Loss_d: 8.8942e-02 | Loss_e: 1.1929e-01 | Loss_b: 6.0737e-03\n",
            "Step: 220 | Loss: 4.7944e-02 | Loss_d: 7.4678e-02 | Loss_e: 1.3959e-01 | Loss_b: 8.7733e-03\n",
            "Step: 240 | Loss: 4.3050e-02 | Loss_d: 6.6126e-02 | Loss_e: 1.4045e-01 | Loss_b: 9.6096e-03\n",
            "Step: 260 | Loss: 3.9121e-02 | Loss_d: 5.9698e-02 | Loss_e: 1.3332e-01 | Loss_b: 9.8252e-03\n",
            "Step: 280 | Loss: 3.5380e-02 | Loss_d: 5.4178e-02 | Loss_e: 1.1208e-01 | Loss_b: 1.1237e-02\n",
            "Step: 300 | Loss: 3.2619e-02 | Loss_d: 5.0169e-02 | Loss_e: 9.5940e-02 | Loss_b: 1.1859e-02\n",
            "Step: 320 | Loss: 2.8515e-02 | Loss_d: 4.3547e-02 | Loss_e: 8.6663e-02 | Loss_b: 1.2076e-02\n",
            "Step: 340 | Loss: 2.6054e-02 | Loss_d: 3.9800e-02 | Loss_e: 7.7777e-02 | Loss_b: 1.1614e-02\n",
            "Step: 360 | Loss: 2.3926e-02 | Loss_d: 3.6707e-02 | Loss_e: 6.6744e-02 | Loss_b: 1.1438e-02\n",
            "Step: 380 | Loss: 2.1937e-02 | Loss_d: 3.3609e-02 | Loss_e: 6.0033e-02 | Loss_b: 1.1539e-02\n",
            "Step: 400 | Loss: 1.9925e-02 | Loss_d: 3.0508e-02 | Loss_e: 5.1475e-02 | Loss_b: 1.2178e-02\n",
            "Step: 420 | Loss: 1.8847e-02 | Loss_d: 2.9094e-02 | Loss_e: 4.1691e-02 | Loss_b: 1.2659e-02\n",
            "Step: 440 | Loss: 1.7443e-02 | Loss_d: 2.7084e-02 | Loss_e: 3.2483e-02 | Loss_b: 1.3192e-02\n",
            "Step: 460 | Loss: 1.6108e-02 | Loss_d: 2.5255e-02 | Loss_e: 2.6491e-02 | Loss_b: 1.1509e-02\n",
            "Step: 480 | Loss: 1.5072e-02 | Loss_d: 2.3679e-02 | Loss_e: 2.2855e-02 | Loss_b: 1.1245e-02\n",
            "Step: 500 | Loss: 1.4312e-02 | Loss_d: 2.2540e-02 | Loss_e: 2.1217e-02 | Loss_b: 1.0364e-02\n",
            "Step: 520 | Loss: 1.4161e-02 | Loss_d: 2.2384e-02 | Loss_e: 1.8761e-02 | Loss_b: 1.0555e-02\n",
            "Step: 540 | Loss: 1.3911e-02 | Loss_d: 2.2081e-02 | Loss_e: 1.7667e-02 | Loss_b: 9.8394e-03\n",
            "Step: 560 | Loss: 1.3370e-02 | Loss_d: 2.1242e-02 | Loss_e: 1.5610e-02 | Loss_b: 9.9399e-03\n",
            "Step: 580 | Loss: 1.3501e-02 | Loss_d: 2.1526e-02 | Loss_e: 1.4926e-02 | Loss_b: 9.6940e-03\n",
            "Step: 600 | Loss: 1.3135e-02 | Loss_d: 2.0928e-02 | Loss_e: 1.4281e-02 | Loss_b: 9.6903e-03\n",
            "Step: 620 | Loss: 1.2954e-02 | Loss_d: 2.0754e-02 | Loss_e: 1.3498e-02 | Loss_b: 8.7076e-03\n",
            "Step: 640 | Loss: 1.2713e-02 | Loss_d: 2.0381e-02 | Loss_e: 1.2385e-02 | Loss_b: 8.8505e-03\n",
            "Step: 660 | Loss: 1.2466e-02 | Loss_d: 1.9910e-02 | Loss_e: 1.2300e-02 | Loss_b: 9.3556e-03\n",
            "Step: 680 | Loss: 1.2399e-02 | Loss_d: 1.9987e-02 | Loss_e: 1.1386e-02 | Loss_b: 7.8776e-03\n",
            "Step: 700 | Loss: 1.1978e-02 | Loss_d: 1.9377e-02 | Loss_e: 1.1131e-02 | Loss_b: 6.8596e-03\n",
            "Step: 720 | Loss: 1.1990e-02 | Loss_d: 1.9274e-02 | Loss_e: 1.0637e-02 | Loss_b: 8.3524e-03\n",
            "Step: 740 | Loss: 1.1458e-02 | Loss_d: 1.8532e-02 | Loss_e: 1.1171e-02 | Loss_b: 6.3458e-03\n",
            "Step: 760 | Loss: 1.1152e-02 | Loss_d: 1.7980e-02 | Loss_e: 1.1260e-02 | Loss_b: 6.5552e-03\n",
            "Step: 780 | Loss: 1.1415e-02 | Loss_d: 1.8356e-02 | Loss_e: 1.1138e-02 | Loss_b: 7.3898e-03\n",
            "Step: 800 | Loss: 1.1319e-02 | Loss_d: 1.8248e-02 | Loss_e: 1.1121e-02 | Loss_b: 6.8164e-03\n",
            "Step: 820 | Loss: 1.1038e-02 | Loss_d: 1.7690e-02 | Loss_e: 1.0905e-02 | Loss_b: 7.6622e-03\n",
            "Step: 840 | Loss: 1.0785e-02 | Loss_d: 1.7189e-02 | Loss_e: 1.2192e-02 | Loss_b: 7.6789e-03\n",
            "Step: 860 | Loss: 1.0921e-02 | Loss_d: 1.7622e-02 | Loss_e: 1.2025e-02 | Loss_b: 5.7764e-03\n",
            "Step: 880 | Loss: 1.0292e-02 | Loss_d: 1.6420e-02 | Loss_e: 1.2328e-02 | Loss_b: 6.8070e-03\n",
            "Step: 900 | Loss: 1.0090e-02 | Loss_d: 1.6058e-02 | Loss_e: 1.2503e-02 | Loss_b: 6.8660e-03\n",
            "Step: 920 | Loss: 9.9932e-03 | Loss_d: 1.5951e-02 | Loss_e: 1.2943e-02 | Loss_b: 6.0535e-03\n",
            "Step: 940 | Loss: 1.0265e-02 | Loss_d: 1.6326e-02 | Loss_e: 1.3516e-02 | Loss_b: 6.6939e-03\n",
            "Step: 960 | Loss: 9.7683e-03 | Loss_d: 1.5523e-02 | Loss_e: 1.3912e-02 | Loss_b: 5.9743e-03\n",
            "Step: 980 | Loss: 9.7985e-03 | Loss_d: 1.5634e-02 | Loss_e: 1.4178e-02 | Loss_b: 5.2541e-03\n",
            "Step: 1000 | Loss: 9.5202e-03 | Loss_d: 1.5152e-02 | Loss_e: 1.4414e-02 | Loss_b: 5.1615e-03\n",
            "Step: 1020 | Loss: 9.5670e-03 | Loss_d: 1.5181e-02 | Loss_e: 1.4833e-02 | Loss_b: 5.4707e-03\n",
            "Step: 1040 | Loss: 9.5854e-03 | Loss_d: 1.5236e-02 | Loss_e: 1.5156e-02 | Loss_b: 5.0705e-03\n",
            "Step: 1060 | Loss: 9.0350e-03 | Loss_d: 1.4316e-02 | Loss_e: 1.4931e-02 | Loss_b: 4.9043e-03\n",
            "Step: 1080 | Loss: 9.0633e-03 | Loss_d: 1.4332e-02 | Loss_e: 1.4819e-02 | Loss_b: 5.2881e-03\n",
            "Step: 1100 | Loss: 8.8710e-03 | Loss_d: 1.4067e-02 | Loss_e: 1.5386e-02 | Loss_b: 4.3514e-03\n",
            "Step: 1120 | Loss: 8.5324e-03 | Loss_d: 1.3455e-02 | Loss_e: 1.5465e-02 | Loss_b: 4.5976e-03\n",
            "Step: 1140 | Loss: 8.7537e-03 | Loss_d: 1.3891e-02 | Loss_e: 1.4816e-02 | Loss_b: 4.3719e-03\n",
            "Step: 1160 | Loss: 9.0492e-03 | Loss_d: 1.4345e-02 | Loss_e: 1.5413e-02 | Loss_b: 4.6189e-03\n",
            "Step: 1180 | Loss: 8.6932e-03 | Loss_d: 1.3730e-02 | Loss_e: 1.4949e-02 | Loss_b: 4.8797e-03\n",
            "Step: 1200 | Loss: 8.2763e-03 | Loss_d: 1.3147e-02 | Loss_e: 1.4447e-02 | Loss_b: 3.7829e-03\n",
            "Step: 1220 | Loss: 8.3924e-03 | Loss_d: 1.3294e-02 | Loss_e: 1.5451e-02 | Loss_b: 3.8036e-03\n",
            "Step: 1240 | Loss: 8.2587e-03 | Loss_d: 1.3009e-02 | Loss_e: 1.5258e-02 | Loss_b: 4.4518e-03\n",
            "Step: 1260 | Loss: 8.1537e-03 | Loss_d: 1.2923e-02 | Loss_e: 1.5532e-02 | Loss_b: 3.3659e-03\n",
            "Step: 1280 | Loss: 8.1953e-03 | Loss_d: 1.2961e-02 | Loss_e: 1.5279e-02 | Loss_b: 3.8344e-03\n",
            "Step: 1300 | Loss: 8.0302e-03 | Loss_d: 1.2615e-02 | Loss_e: 1.5439e-02 | Loss_b: 4.3668e-03\n",
            "Step: 1320 | Loss: 7.7759e-03 | Loss_d: 1.2184e-02 | Loss_e: 1.5819e-02 | Loss_b: 4.1123e-03\n",
            "Step: 1340 | Loss: 8.0839e-03 | Loss_d: 1.2727e-02 | Loss_e: 1.6314e-02 | Loss_b: 3.7392e-03\n",
            "Step: 1360 | Loss: 7.9538e-03 | Loss_d: 1.2474e-02 | Loss_e: 1.6197e-02 | Loss_b: 4.0800e-03\n",
            "Step: 1380 | Loss: 7.6293e-03 | Loss_d: 1.1933e-02 | Loss_e: 1.6313e-02 | Loss_b: 3.8459e-03\n",
            "Step: 1400 | Loss: 8.0455e-03 | Loss_d: 1.2667e-02 | Loss_e: 1.6770e-02 | Loss_b: 3.4419e-03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training using L-BFGS optimizer\n",
        "Extra training using L-BFGS to reach higher accuracy\n",
        "\n",
        "Recommended number of iterations: 10000"
      ],
      "metadata": {
        "id": "Ix3Kcs7Y_2rw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the training iteration\n",
        "epoch2 = 10000\n",
        "# re-sample the data and collocation points\n",
        "data_l = dataf_l(key_lbfgs[1])\n",
        "\n",
        "# training with L-bfgs\n",
        "trained_params2, loss2 = lbfgs_optimizer(NN_loss, trained_params, data_l, epoch2)"
      ],
      "metadata": {
        "id": "7gMWY2C8oAIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing the output of trained networks\n",
        "\n",
        "Compute the solution variables and equation residue at high-resolution grids"
      ],
      "metadata": {
        "id": "otVSMVBeoUs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function of solution and equation residues based on trained networks\n",
        "f_u = lambda x: pred_u(trained_params2, x)\n",
        "f_gu = lambda x: vectgrad(f_u, x)[0][:, 0:6]\n",
        "f_eqn = lambda x: gov_eqn(f_u, x, scale)\n",
        "\n",
        "# group all the function\n",
        "func_all = (f_u, f_gu, f_eqn)\n",
        "\n",
        "# calculate the solution and equation residue at given grids for visualization\n",
        "results = predict(func_all, data_all)\n"
      ],
      "metadata": {
        "id": "KnDbH7sZoRHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting the results:\n",
        "\n",
        "Compare the synthetic data for either velocity or thickness with the corresponding network approximation"
      ],
      "metadata": {
        "id": "uYW4pLapu508"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "u_g = results['u_g']\n",
        "u = results['u']\n",
        "\n",
        "fig = plt.figure(figsize = [10, 10], dpi = 70)\n",
        "\n",
        "ax = plt.subplot(2,1,1)\n",
        "h = ax.imshow(u_g, interpolation='nearest', cmap='rainbow',\n",
        "              extent=[0., 50000., 0,  80000.],\n",
        "              origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"4%\", pad=0.05)\n",
        "fig.colorbar(h, cax=cax)\n",
        "\n",
        "ax.set_xlabel('$x$', fontsize = 15)\n",
        "ax.set_ylabel('$y\\ $', fontsize = 15, rotation = 0)\n",
        "ax.set_title('Syn. Data $u_g(x,y)$ (m/s)', fontsize = 15)\n",
        "\n",
        "\n",
        "ax2 = plt.subplot(2,1,2)\n",
        "h2 = ax2.imshow(u, interpolation='nearest', cmap='rainbow',\n",
        "              extent=[0., 50000., 0,  80000.],\n",
        "              origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax2)\n",
        "cax = divider.append_axes(\"right\", size=\"4%\", pad=0.05)\n",
        "fig.colorbar(h2, cax=cax)\n",
        "\n",
        "ax2.set_xlabel('$x$', fontsize = 15)\n",
        "ax2.set_ylabel('$y\\ $', fontsize = 15, rotation = 0)\n",
        "ax2.set_title('Network approx. $u(x,y)$ (m/s)', fontsize = 15)\n"
      ],
      "metadata": {
        "id": "wmOpheP3qfn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Showing the inferred viscosity via PINNs for the ice shelf"
      ],
      "metadata": {
        "id": "woLTJcNh4DZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the PINN inference of viscosity\n",
        "mu = results['mu']\n",
        "\n",
        "fig = plt.figure(figsize = [10, 5], dpi = 70)\n",
        "\n",
        "ax = plt.subplot(1,1,1)\n",
        "h = ax.imshow(mu_g, interpolation='nearest', cmap='rainbow',\n",
        "              extent=[0., 50000., 0,  80000.],\n",
        "              origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"4%\", pad=0.05)\n",
        "fig.colorbar(h, cax=cax)\n",
        "\n",
        "ax.set_xlabel('$x$', fontsize = 15)\n",
        "ax.set_ylabel('$y\\ $', fontsize = 15, rotation = 0)\n",
        "ax.set_title('Inferred viscosity $\\mu(x,y)$', fontsize = 15)\n",
        "\n"
      ],
      "metadata": {
        "id": "DOLQMeRkx4gR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}