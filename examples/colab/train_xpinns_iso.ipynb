{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMWPNdkd3OZ5DzMPjt/tuVq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YaoGroup/DIFFICE_jax/blob/main/examples/colab/train_xpinns_iso.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Isotropic viscosity inversion of large ice shelves via extended-PINNs (XPINNS)"
      ],
      "metadata": {
        "id": "cA8RPIy4vSe5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fast running the code requires **GPU hardware** to accelerate.\n",
        "\n",
        "To choose GPU hardware, click the \"Edit\" in the toolbar above and select \"Notebook settings\".\n",
        "\n",
        "In the pop-up window, under the \"Hardware accelerator\", select \"A100 GPU\" (Pro account required or compute unit needed)\n",
        "\n",
        "The inversion of large ice shelves via XPINNs overall required more computational resurce. **Highly recommend** users to choose **A100** GPU for the training. Otherwise, the users might encouter out-of-memory (OOM) issues."
      ],
      "metadata": {
        "id": "tZQ4X2lgfY0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To ensure the code to run correctly in the colab, install the specific version of JAX below.\n",
        "\n",
        "(Note that only using the GPU version of JAX, combined with the selection of GPU hardware as mentioned above, can eventually accelerate the training.)"
      ],
      "metadata": {
        "id": "jlnGqpgzgf_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copying required python functions from the [DIFFICE_jax](https://github.com/YaoGroup/DIFFICE_jax) GitHub repository"
      ],
      "metadata": {
        "id": "7gbfuwvAhaiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install the DIFFICE_jax\n",
        "!pip install DIFFICE_jax\n",
        "\n",
        "# download the data from GitHub respository\n",
        "!wget https://github.com/YaoGroup/DIFFICE_jax/raw/main/examples/real_data/data_xpinns_RnFlch.mat\n"
      ],
      "metadata": {
        "id": "eHc_W9sTUBF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the required JAX library and function file from GitHub repository"
      ],
      "metadata": {
        "id": "9cv4dMKPhmLE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZSe-qjr3Ju2N"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from jax import random, lax\n",
        "from jax.tree_util import tree_map\n",
        "from scipy.io import loadmat\n",
        "import time\n",
        "\n",
        "from diffice_jax import normdata_xpinn, dsample_xpinn\n",
        "from diffice_jax import vectgrad, ssa_iso, dbc_iso\n",
        "from diffice_jax import init_xpinn, solu_xpinn\n",
        "from diffice_jax import loss_iso_xpinn\n",
        "from diffice_jax import predict_xpinn\n",
        "from diffice_jax import adam_opt, lbfgs_opt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting hyperparameters\n",
        "\n",
        "hyper-parameters used for the training. Users are free to modify their value and check their influence on the training results\n"
      ],
      "metadata": {
        "id": "NIvVtcvew_nY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select a random seed\n",
        "seed = 2134\n",
        "key = random.PRNGKey(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# create the subkeys\n",
        "keys = random.split(key, 4)\n",
        "\n",
        "# select the size of neural network\n",
        "n_hl = 5\n",
        "n_unit = 30\n",
        "# set the weights for 1. equation loss, 2. boundary condition loss\n",
        "# 3. matching condition loss and 4. regularization loss\n",
        "lw = [0.05, 0.1, 1, 0.25]\n",
        "\n",
        "# number of sampling points\n",
        "n_smp = 4000    # for velocity data\n",
        "nh_smp = 3500   # for thickness data\n",
        "n_col = 4000    # for collocation points\n",
        "n_cbd = 400     # for boundary condition (calving front)\n",
        "# group all the number of points\n",
        "n_pt = jnp.array([n_smp, nh_smp, n_col, n_cbd], dtype='int32')\n",
        "# double the points for L-BFGS training\n",
        "n_pt2 = n_pt * 2\n"
      ],
      "metadata": {
        "id": "wFBWogcjXcEe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading\n",
        "load and normalize tne observed data before the PINN training"
      ],
      "metadata": {
        "id": "lKV1ytJ8mcbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the synthetic data\n",
        "rawdata = loadmat('data_xpinns_RnFlch.mat')\n",
        "\n",
        "# normalize the remote-sensing data for the XPINNs training\n",
        "data_all, idxgall, posi_all, idxcrop_all = normdata_xpinn(rawdata)\n",
        "# extract the scale information for each variable\n",
        "scale = tree_map(lambda x: data_all[x][4][0:2], idxgall)\n"
      ],
      "metadata": {
        "id": "S0S1O_qHmc3d"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization\n",
        "\n",
        "initialize the neural network and loss function"
      ],
      "metadata": {
        "id": "0X6CNFhfxOL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the weights and biases of the network\n",
        "trained_params = init_xpinn(keys[0], n_hl, n_unit,\n",
        "                             n_sub=len(idxgall))\n",
        "\n",
        "# create the solution function [tuple(callable, callable)]\n",
        "solNN = solu_xpinn(scale)\n",
        "\n",
        "# create the data function for Adam\n",
        "dataf = dsample_xpinn(data_all, idxgall, n_pt)\n",
        "keys_adam = random.split(keys[1], 5)\n",
        "# generate the data\n",
        "data = dataf(keys_adam[0])\n",
        "\n",
        "# create the data function for L-BFGS\n",
        "dataf_l = dsample_xpinn(data_all, idxgall, n_pt2)\n",
        "key_lbfgs = keys[2]\n",
        "\n",
        "# group the gov. eqn and bd cond.\n",
        "eqn_all = (ssa_iso, dbc_iso)\n",
        "# calculate the loss function\n",
        "NN_loss = loss_iso_xpinn(solNN, eqn_all, scale, idxgall, lw)\n",
        "# calculate the initial loss and set it as the loss reference value\n",
        "NN_loss.lref = NN_loss(trained_params, data)[0]\n"
      ],
      "metadata": {
        "id": "fzpbvcDDXunl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network training\n",
        "\n",
        "Since the real ice shelf data has more complicated profile than the synthetic data. 10000 iterations of Adam, followed by another 10000 iterations of L-BFGS can only infer a **very rough** profile of the ice viscosity.\n",
        "\n",
        "To train a high-accurate model, the number of iterations required for both Adam and L-BFGS optimization is more than 100k.\n"
      ],
      "metadata": {
        "id": "6LBYqgjFAuPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the learning rate for Adam\n",
        "lr = 1e-3\n",
        "# set the training iteration\n",
        "epoch1 = 10000\n",
        "\n",
        "# training with Adam with reducing w\n",
        "trained_params, loss1 = adam_opt(\n",
        "    keys_adam[1], NN_loss, trained_params, dataf, epoch1, lr=lr)\n"
      ],
      "metadata": {
        "id": "mNVBZd_OYDAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cb2db07-4934-43b9-f895-1962a2092a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Step: 100 | Loss: 8.3111e-02 | Loss_d: 1.4055e-01 | Loss_e: 2.4170e-02 | Loss_b: 2.1272e-03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extra training using L-BFGS to reach higher accuracy\n",
        "\n",
        "Recommended number of iterations: 10000"
      ],
      "metadata": {
        "id": "Ix3Kcs7Y_2rw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the training iteration\n",
        "epoch2 = 10000\n",
        "# re-sample the data and collocation points\n",
        "data_l = dataf_l(key_lbfgs[1])\n",
        "\n",
        "# training with L-bfgs\n",
        "trained_params2, loss2 = lbfgs_opt(NN_loss, trained_params, data_l, epoch2)\n"
      ],
      "metadata": {
        "id": "7gMWY2C8oAIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction\n",
        "\n",
        "Compute the solution variables and equation residue at high-resolution grids"
      ],
      "metadata": {
        "id": "otVSMVBeoUs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the function for trained solution and equation residues\n",
        "f_u = lambda x, idx: solNN[0](trained_params2, x, idx)\n",
        "\n",
        "# group all the function\n",
        "func_all = (f_u, ssa_iso)\n",
        "\n",
        "# calculate the solution and equation residue at given grids for visualization\n",
        "results = predict_xpinn(func_all, data_all, posi_all, idxcrop_all, idxgall)\n"
      ],
      "metadata": {
        "id": "KnDbH7sZoRHu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting the results:\n",
        "\n",
        "Compare the synthetic data for either velocity or thickness with the corresponding network approximation"
      ],
      "metadata": {
        "id": "uYW4pLapu508"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "u_g = results['u_g']\n",
        "u = results['u']\n",
        "\n",
        "fig = plt.figure(figsize = [10, 10], dpi = 70)\n",
        "\n",
        "ax = plt.subplot(2,1,1)\n",
        "h = ax.imshow(u_g, interpolation='nearest', cmap='rainbow',\n",
        "              extent=[0., 50000., 0,  80000.],\n",
        "              origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"4%\", pad=0.05)\n",
        "fig.colorbar(h, cax=cax)\n",
        "\n",
        "ax.set_xlabel('$x$', fontsize = 15)\n",
        "ax.set_ylabel('$y\\ $', fontsize = 15, rotation = 0)\n",
        "ax.set_title('Syn. Data $u_g(x,y)$ (m/s)', fontsize = 15)\n",
        "\n",
        "\n",
        "ax2 = plt.subplot(2,1,2)\n",
        "h2 = ax2.imshow(u, interpolation='nearest', cmap='rainbow',\n",
        "              extent=[0., 50000., 0,  80000.],\n",
        "              origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax2)\n",
        "cax = divider.append_axes(\"right\", size=\"4%\", pad=0.05)\n",
        "fig.colorbar(h2, cax=cax)\n",
        "\n",
        "ax2.set_xlabel('$x$', fontsize = 15)\n",
        "ax2.set_ylabel('$y\\ $', fontsize = 15, rotation = 0)\n",
        "ax2.set_title('Network approx. $u(x,y)$ (m/s)', fontsize = 15)\n"
      ],
      "metadata": {
        "id": "wmOpheP3qfn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Showing the inferred viscosity via PINNs for the ice shelf"
      ],
      "metadata": {
        "id": "woLTJcNh4DZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the PINN inference of viscosity\n",
        "mu = results['mu']\n",
        "\n",
        "fig = plt.figure(figsize = [10, 5], dpi = 70)\n",
        "\n",
        "ax = plt.subplot(1,1,1)\n",
        "h = ax.imshow(mu, interpolation='nearest', cmap='rainbow',\n",
        "              extent=[0., 50000., 0,  80000.],\n",
        "              origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"4%\", pad=0.05)\n",
        "fig.colorbar(h, cax=cax)\n",
        "\n",
        "ax.set_xlabel('$x$', fontsize = 15)\n",
        "ax.set_ylabel('$y\\ $', fontsize = 15, rotation = 0)\n",
        "ax.set_title('Inferred viscosity $\\mu(x,y)$', fontsize = 15)\n",
        "\n"
      ],
      "metadata": {
        "id": "DOLQMeRkx4gR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}