{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPMaHXVWfsdEiINl+lVlDe3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YaoGroup/DIFFICE_jax/blob/main/tutorial/train_syndata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial: Viscosity inversion from synthetic data via PINNs"
      ],
      "metadata": {
        "id": "cA8RPIy4vSe5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fast running the code requires **GPU hardware** to accelerate.\n",
        "\n",
        "To choose GPU hardware, click the \"Edit\" in the toolbar above and select \"Notebook settings\".\n",
        "\n",
        "In the pop-up window, under the \"Hardware accelerator\", select \"T4 GPU\""
      ],
      "metadata": {
        "id": "tZQ4X2lgfY0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To ensure the code to run correctly in the colab, install the specific version of JAX below.\n",
        "\n",
        "(Note that only using the GPU version of JAX, combined with the selection of GPU hardware as mentioned above, can eventually accelerate the training.)"
      ],
      "metadata": {
        "id": "jlnGqpgzgf_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install specific version (0.4.23) of JAX and Jaxlib\n",
        "!pip install --upgrade jax==0.4.23 jaxlib==0.4.23+cuda12.cudnn89 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "\n",
        "# Verify the installed version\n",
        "import jax\n",
        "print(jax.__version__)"
      ],
      "metadata": {
        "id": "ei2r5MmucPiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copying required python functions from the [DIFFICE_jax](https://github.com/YaoGroup/DIFFICE_jax) GitHub repository"
      ],
      "metadata": {
        "id": "7gbfuwvAhaiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the GitHub respository\n",
        "!git clone https://github.com/YaoGroup/DIFFICE_jax\n",
        "\n",
        "# download the data from GitHub respository\n",
        "!wget https://github.com/YaoGroup/DIFFICE_jax/blob/main/tutorial/COMSOL/SynData_exp1.mat"
      ],
      "metadata": {
        "id": "eHc_W9sTUBF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the required JAX library and function file from GitHub repository"
      ],
      "metadata": {
        "id": "9cv4dMKPhmLE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZSe-qjr3Ju2N"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from jax import random\n",
        "from jax.tree_util import tree_map\n",
        "from scipy.io import loadmat\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "from DIFFICE_jax.data.pinns.preprocessing import normalize_data\n",
        "from DIFFICE_jax.data.pinns.sampling import data_sample_create\n",
        "from DIFFICE_jax.equation.ssa_eqn_iso import vectgrad, gov_eqn, front_eqn\n",
        "from DIFFICE_jax.model.pinns.initialization import init_pinns\n",
        "from DIFFICE_jax.model.pinns.networks import solu_create\n",
        "from DIFFICE_jax.model.pinns.loss import loss_iso_create\n",
        "from DIFFICE_jax.model.pinns.prediction import predict\n",
        "from DIFFICE_jax.optimizer.optimization import adam_optimizer, lbfgs_optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting hyperparameters\n",
        "\n",
        "hyper-parameters used for the training. Users are free to modify their value and check their influence on the training results\n"
      ],
      "metadata": {
        "id": "NIvVtcvew_nY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select a random seed\n",
        "seed = 2134\n",
        "key = random.PRNGKey(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# create the subkeys\n",
        "keys = random.split(key, 4)\n",
        "\n",
        "# select the size of neural network\n",
        "n_hl = 6  # number of hidden layers\n",
        "n_unit = 30  # number of units in each hidden layer\n",
        "\n",
        "# select the weight for the equation and boundary conditions\n",
        "lw = [0.05, 0.1]\n",
        "\n",
        "# number of sampling points\n",
        "n_smp = 4000    # for velocity data\n",
        "nh_smp = 3000   # for thickness data\n",
        "n_col = 4000    # for collocation points\n",
        "n_cbd = 600     # for boundary condition (calving front)\n",
        "\n",
        "# group all the number of points\n",
        "n_pt = jnp.array([n_smp, nh_smp, n_col, n_cbd], dtype='int32')\n",
        "# double the points for L-BFGS training\n",
        "n_pt2 = n_pt * 2\n"
      ],
      "metadata": {
        "id": "wFBWogcjXcEe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading\n",
        "load and normalize tne observed data before the PINN training"
      ],
      "metadata": {
        "id": "xmbVLe6I-2XC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the synthetic data\n",
        "rawdata = loadmat('SynData_exp1.mat')\n",
        "\n",
        "# normalize the synthetic data for the PINN traing\n",
        "data_all = normalize_data(rawdata)\n",
        "# extract the scale information for each variable\n",
        "scale = data_all[4][0:2]\n"
      ],
      "metadata": {
        "id": "30v7bVLm-1pU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization\n",
        "\n",
        "initialize the neural network and loss function"
      ],
      "metadata": {
        "id": "0X6CNFhfxOL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the weights and biases of the network\n",
        "trained_params = init_pinns(keys[0], n_hl, n_unit)\n",
        "\n",
        "# create the solution function\n",
        "pred_u = solu_create()\n",
        "\n",
        "# create the data sampling function for Adam training\n",
        "dataf = data_sample_create(data_all, n_pt)\n",
        "keys_adam = random.split(keys[1], 5)\n",
        "# generate the data\n",
        "data = dataf(keys_adam[0])\n",
        "\n",
        "# create the data sampling function for L-BFGS training\n",
        "dataf_l = data_sample_create(data_all, n_pt2)\n",
        "key_lbfgs = random.split(keys[2], 5)\n",
        "\n",
        "\n",
        "# group the gov. eqn and bdry cond.\n",
        "eqn_all = (gov_eqn, front_eqn)\n",
        "# create the loss function\n",
        "NN_loss = loss_iso_create(pred_u, eqn_all, scale, lw)\n",
        "# calculate the initial loss and set it as the reference value for loss\n",
        "NN_loss.lref = NN_loss(trained_params, data)[0]\n"
      ],
      "metadata": {
        "id": "fzpbvcDDXunl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network training\n",
        "\n",
        "Require at least 8000 iterations of Adam,\n",
        "plus 8000 iterations of L-BFGS later\n",
        "for a relatively accurate trained model\n"
      ],
      "metadata": {
        "id": "6LBYqgjFAuPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the learning rate for Adam\n",
        "lr = 1e-3\n",
        "# set the training iteration\n",
        "epoch1 = 10000\n",
        "\n",
        "# training with Adam\n",
        "trained_params, loss1 = adam_optimizer(keys_adam[1], NN_loss, trained_params, dataf, epoch1, lr=lr)\n"
      ],
      "metadata": {
        "id": "mNVBZd_OYDAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extra training using L-BFGS to reach higher accuracy\n",
        "\n",
        "Recommended number of iterations: 8000"
      ],
      "metadata": {
        "id": "Ix3Kcs7Y_2rw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the training iteration\n",
        "epoch2 = 10000\n",
        "# re-sample the data and collocation points\n",
        "data_l = dataf_l(key_lbfgs[1])\n",
        "\n",
        "# training with L-bfgs\n",
        "trained_params2, loss2 = lbfgs_optimizer(NN_loss, trained_params, data_l, epoch2)"
      ],
      "metadata": {
        "id": "7gMWY2C8oAIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction\n",
        "\n",
        "Compute the solution variables and equation residue at high-resolution grids"
      ],
      "metadata": {
        "id": "otVSMVBeoUs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function of solution and equation residues based on trained networks\n",
        "f_u = lambda x: pred_u(trained_params2, x)\n",
        "f_gu = lambda x: vectgrad(f_u, x)[0][:, 0:6]\n",
        "\n",
        "# group all the function\n",
        "func_all = (f_u, f_gu, gov_eqn)\n",
        "\n",
        "# calculate the solution and equation residue at given grids for visualization\n",
        "results = predict(func_all, data_all)\n"
      ],
      "metadata": {
        "id": "KnDbH7sZoRHu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting the results:\n",
        "\n",
        "Compare the synthetic data for either velocity or thickness with the corresponding network approximation"
      ],
      "metadata": {
        "id": "uYW4pLapu508"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "u_g = results['u_g']\n",
        "u = results['u']\n",
        "\n",
        "fig = plt.figure(figsize = [10, 10], dpi = 70)\n",
        "\n",
        "ax = plt.subplot(2,1,1)\n",
        "h = ax.imshow(u_g, interpolation='nearest', cmap='rainbow',\n",
        "              extent=[0., 50000., 0,  80000.],\n",
        "              origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"4%\", pad=0.05)\n",
        "fig.colorbar(h, cax=cax)\n",
        "\n",
        "ax.set_xlabel('$x$', fontsize = 15)\n",
        "ax.set_ylabel('$y\\ $', fontsize = 15, rotation = 0)\n",
        "ax.set_title('Syn. Data $u_g(x,y)$ (m/s)', fontsize = 15)\n",
        "\n",
        "\n",
        "ax2 = plt.subplot(2,1,2)\n",
        "h2 = ax2.imshow(u, interpolation='nearest', cmap='rainbow',\n",
        "              extent=[0., 50000., 0,  80000.],\n",
        "              origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax2)\n",
        "cax = divider.append_axes(\"right\", size=\"4%\", pad=0.05)\n",
        "fig.colorbar(h2, cax=cax)\n",
        "\n",
        "ax2.set_xlabel('$x$', fontsize = 15)\n",
        "ax2.set_ylabel('$y\\ $', fontsize = 15, rotation = 0)\n",
        "ax2.set_title('Network approx. $u(x,y)$ (m/s)', fontsize = 15)\n"
      ],
      "metadata": {
        "id": "wmOpheP3qfn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the given viscosity profile with inferred viscosity via PINNs"
      ],
      "metadata": {
        "id": "woLTJcNh4DZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the given viscosity (ground truth)\n",
        "mu_g = rawdata['mud']\n",
        "# load the PINN inference of viscosity\n",
        "mu = results['mu']\n",
        "\n",
        "fig = plt.figure(figsize = [10, 10], dpi = 70)\n",
        "\n",
        "ax = plt.subplot(2,1,1)\n",
        "h = ax.imshow(mu_g, interpolation='nearest', cmap='rainbow',\n",
        "              extent=[0., 50000., 0,  80000.], clim=[1.5e13, 7.5e13],\n",
        "              origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"4%\", pad=0.05)\n",
        "fig.colorbar(h, cax=cax)\n",
        "\n",
        "ax.set_xlabel('$x$', fontsize = 15)\n",
        "ax.set_ylabel('$y\\ $', fontsize = 15, rotation = 0)\n",
        "ax.set_title('Given viscosity $\\mu_g(x,y)$', fontsize = 15)\n",
        "\n",
        "\n",
        "ax2 = plt.subplot(2,1,2)\n",
        "h2 = ax2.imshow(mu, interpolation='nearest', cmap='rainbow',\n",
        "              extent=[0., 50000., 0,  80000.], clim=[1.5e13, 7.5e13],\n",
        "              origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax2)\n",
        "cax = divider.append_axes(\"right\", size=\"4%\", pad=0.05)\n",
        "fig.colorbar(h2, cax=cax)\n",
        "\n",
        "ax2.set_xlabel('$x$', fontsize = 15)\n",
        "ax2.set_ylabel('$y\\ $', fontsize = 15, rotation = 0)\n",
        "ax2.set_title('PINN inversion $\\mu(x,y)$', fontsize = 15)\n"
      ],
      "metadata": {
        "id": "DOLQMeRkx4gR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}