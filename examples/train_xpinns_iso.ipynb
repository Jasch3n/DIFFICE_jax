{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNMRppy0Ag7SZRjdAit9OS6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YaoGroup/DIFFICE_jax/blob/main/examples/train_xpinns_iso.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Isotropic viscosity inversion of large ice shelves via extended-PINNs (XPINNS)"
      ],
      "metadata": {
        "id": "cA8RPIy4vSe5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fast running the code requires **GPU hardware** to accelerate.\n",
        "\n",
        "To choose GPU hardware, click the \"Edit\" in the toolbar above and select \"Notebook settings\".\n",
        "\n",
        "In the pop-up window, under the \"Hardware accelerator\", select \"A100 GPU\" (Pro account required or compute unit needed)\n",
        "\n",
        "The inversion of large ice shelves via XPINNs overall required more computational resurce. **Highly recommend** users to choose **A100** GPU for the training. Otherwise, the users might encouter out-of-memory (OOM) issues."
      ],
      "metadata": {
        "id": "tZQ4X2lgfY0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To ensure the code to run correctly in the colab, install the specific version of JAX below.\n",
        "\n",
        "(Note that only using the GPU version of JAX, combined with the selection of GPU hardware as mentioned above, can eventually accelerate the training.)"
      ],
      "metadata": {
        "id": "jlnGqpgzgf_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install specific version (0.4.23) of JAX and Jaxlib\n",
        "!pip install --upgrade jax==0.4.23 jaxlib==0.4.23+cuda12.cudnn89 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "\n",
        "# Verify the installed version\n",
        "import jax\n",
        "print(jax.__version__)"
      ],
      "metadata": {
        "id": "ei2r5MmucPiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copying required python functions from the [DIFFICE_jax](https://github.com/YaoGroup/DIFFICE_jax) GitHub repository"
      ],
      "metadata": {
        "id": "7gbfuwvAhaiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the GitHub respository\n",
        "!git clone https://github.com/YaoGroup/DIFFICE_jax\n",
        "\n",
        "# add the path of the github folder that contains the data\n",
        "import os\n",
        "os.chdir('DIFFICE_jax/data')\n"
      ],
      "metadata": {
        "id": "eHc_W9sTUBF6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67414092-d6cb-4bf4-f8fd-826a86ff683e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DIFFICE_jax'...\n",
            "remote: Enumerating objects: 991, done.\u001b[K\n",
            "remote: Counting objects: 100% (407/407), done.\u001b[K\n",
            "remote: Compressing objects: 100% (227/227), done.\u001b[K\n",
            "remote: Total 991 (delta 325), reused 239 (delta 180), pack-reused 584\u001b[K\n",
            "Receiving objects: 100% (991/991), 91.60 MiB | 16.27 MiB/s, done.\n",
            "Resolving deltas: 100% (583/583), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the required JAX library and function file from GitHub repository"
      ],
      "metadata": {
        "id": "9cv4dMKPhmLE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSe-qjr3Ju2N"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from jax import random, lax\n",
        "from jax.tree_util import tree_map\n",
        "from scipy.io import loadmat\n",
        "import time\n",
        "\n",
        "from DIFFICE_jax.data.xpinns.preprocessing import normalize_data\n",
        "from DIFFICE_jax.data.xpinns.sampling import data_sample_create\n",
        "from DIFFICE_jax.equation.ssa_eqn_iso import vectgrad, gov_eqn, front_eqn\n",
        "from DIFFICE_jax.model.xpinns.initialization import init_xpinns\n",
        "from DIFFICE_jax.model.xpinns.networks import solu_create\n",
        "from DIFFICE_jax.model.xpinns.loss import loss_iso_create\n",
        "from DIFFICE_jax.model.xpinns.prediction import predict\n",
        "from DIFFICE_jax.optimizer.optimization import adam_optimizer, lbfgs_optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting hyperparameters\n",
        "\n",
        "hyper-parameters used for the training. Users are free to modify their value and check their influence on the training results\n"
      ],
      "metadata": {
        "id": "NIvVtcvew_nY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select a random seed\n",
        "seed = 2134\n",
        "key = random.PRNGKey(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# create the subkeys\n",
        "keys = random.split(key, 4)\n",
        "\n",
        "# select the size of neural network\n",
        "n_hl = 5\n",
        "n_unit = 30\n",
        "# set the weights for 1. equation loss, 2. boundary condition loss\n",
        "# 3. matching condition loss and 4. regularization loss\n",
        "lw = [0.05, 0.1, 1, 0.25]\n",
        "\n",
        "# number of sampling points\n",
        "n_smp = 4000    # for velocity data\n",
        "nh_smp = 3500   # for thickness data\n",
        "n_col = 4000    # for collocation points\n",
        "n_cbd = 400     # for boundary condition (calving front)\n",
        "# group all the number of points\n",
        "n_pt = jnp.array([n_smp, nh_smp, n_col, n_cbd], dtype='int32')\n",
        "# double the points for L-BFGS training\n",
        "n_pt2 = n_pt * 2\n"
      ],
      "metadata": {
        "id": "wFBWogcjXcEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading\n",
        "load and normalize tne observed data before the PINN training"
      ],
      "metadata": {
        "id": "lKV1ytJ8mcbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the synthetic data\n",
        "rawdata = loadmat('data_xpinns_RnFlch.mat')\n",
        "\n",
        "# normalize the remote-sensing data for the XPINNs training\n",
        "data_all, idxgall, posi_all, idxcrop_all = normalize_data(rawdata)\n",
        "# extract the scale information for each variable\n",
        "scale = tree_map(lambda x: data_all[x][4][0:2], idxgall)\n"
      ],
      "metadata": {
        "id": "S0S1O_qHmc3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization\n",
        "\n",
        "initialize the neural network and loss function"
      ],
      "metadata": {
        "id": "0X6CNFhfxOL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the weights and biases of the network\n",
        "trained_params = init_xpinns(keys[0], n_hl, n_unit,\n",
        "                             n_sub=len(idxgall))\n",
        "\n",
        "# create the solution function [tuple(callable, callable)]\n",
        "solNN = solu_create(scale)\n",
        "\n",
        "# create the data function for Adam\n",
        "dataf = data_sample_create(data_all, idxgall, n_pt)\n",
        "keys_adam = random.split(keys[1], 5)\n",
        "# generate the data\n",
        "data = dataf(keys_adam[0])\n",
        "\n",
        "# create the data function for L-BFGS\n",
        "dataf_l = data_sample_create(data_all, idxgall, n_pt2)\n",
        "key_lbfgs = keys[2]\n",
        "\n",
        "# group the gov. eqn and bd cond.\n",
        "eqn_all = (gov_eqn, front_eqn)\n",
        "# calculate the loss function\n",
        "NN_loss = loss_iso_create(solNN, eqn_all, scale, idxgall, lw)\n",
        "# calculate the initial loss and set it as the loss reference value\n",
        "NN_loss.lref = NN_loss(trained_params, data)[0]\n"
      ],
      "metadata": {
        "id": "fzpbvcDDXunl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network training\n",
        "\n",
        "Since the real ice shelf data has more complicated profile than the synthetic data. 10000 iterations of Adam, followed by another 10000 iterations of L-BFGS can only infer a **very rough** profile of the ice viscosity.\n",
        "\n",
        "To train a high-accurate model, the number of iterations required for both Adam and L-BFGS optimization is more than 100k.\n"
      ],
      "metadata": {
        "id": "6LBYqgjFAuPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the learning rate for Adam\n",
        "lr = 1e-3\n",
        "# set the training iteration\n",
        "epoch1 = 10000\n",
        "\n",
        "# training with Adam with reducing w\n",
        "trained_params, loss1 = adam_optimizer(\n",
        "    keys_adam[1], NN_loss, trained_params, dataf, epoch1, lr=lr)\n"
      ],
      "metadata": {
        "id": "mNVBZd_OYDAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cb2db07-4934-43b9-f895-1962a2092a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Step: 100 | Loss: 8.3111e-02 | Loss_d: 1.4055e-01 | Loss_e: 2.4170e-02 | Loss_b: 2.1272e-03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extra training using L-BFGS to reach higher accuracy\n",
        "\n",
        "Recommended number of iterations: 10000"
      ],
      "metadata": {
        "id": "Ix3Kcs7Y_2rw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the training iteration\n",
        "epoch2 = 10000\n",
        "# re-sample the data and collocation points\n",
        "data_l = dataf_l(key_lbfgs[1])\n",
        "\n",
        "# training with L-bfgs\n",
        "trained_params2, loss2 = lbfgs_optimizer(NN_loss, trained_params, data_l, epoch2)\n"
      ],
      "metadata": {
        "id": "7gMWY2C8oAIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction\n",
        "\n",
        "Compute the solution variables and equation residue at high-resolution grids"
      ],
      "metadata": {
        "id": "otVSMVBeoUs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the function for trained solution and equation residues\n",
        "f_u = lambda x, idx: solNN[0](trained_params, x, idx)\n",
        "\n",
        "# group all the function\n",
        "func_all = (f_u, gov_eqn)\n",
        "\n",
        "# calculate the solution and equation residue at given grids for visualization\n",
        "results = predict(func_all, data_all, posi_all, idxcrop_all, idxgall)\n"
      ],
      "metadata": {
        "id": "KnDbH7sZoRHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting the results:\n",
        "\n",
        "Compare the synthetic data for either velocity or thickness with the corresponding network approximation"
      ],
      "metadata": {
        "id": "uYW4pLapu508"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "u_g = results['u_g']\n",
        "u = results['u']\n",
        "\n",
        "fig = plt.figure(figsize = [10, 10], dpi = 70)\n",
        "\n",
        "ax = plt.subplot(2,1,1)\n",
        "h = ax.imshow(u_g, interpolation='nearest', cmap='rainbow',\n",
        "              extent=[0., 50000., 0,  80000.],\n",
        "              origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"4%\", pad=0.05)\n",
        "fig.colorbar(h, cax=cax)\n",
        "\n",
        "ax.set_xlabel('$x$', fontsize = 15)\n",
        "ax.set_ylabel('$y\\ $', fontsize = 15, rotation = 0)\n",
        "ax.set_title('Syn. Data $u_g(x,y)$ (m/s)', fontsize = 15)\n",
        "\n",
        "\n",
        "ax2 = plt.subplot(2,1,2)\n",
        "h2 = ax2.imshow(u, interpolation='nearest', cmap='rainbow',\n",
        "              extent=[0., 50000., 0,  80000.],\n",
        "              origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax2)\n",
        "cax = divider.append_axes(\"right\", size=\"4%\", pad=0.05)\n",
        "fig.colorbar(h2, cax=cax)\n",
        "\n",
        "ax2.set_xlabel('$x$', fontsize = 15)\n",
        "ax2.set_ylabel('$y\\ $', fontsize = 15, rotation = 0)\n",
        "ax2.set_title('Network approx. $u(x,y)$ (m/s)', fontsize = 15)\n"
      ],
      "metadata": {
        "id": "wmOpheP3qfn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Showing the inferred viscosity via PINNs for the ice shelf"
      ],
      "metadata": {
        "id": "woLTJcNh4DZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the PINN inference of viscosity\n",
        "mu = results['mu']\n",
        "\n",
        "fig = plt.figure(figsize = [10, 5], dpi = 70)\n",
        "\n",
        "ax = plt.subplot(1,1,1)\n",
        "h = ax.imshow(mu, interpolation='nearest', cmap='rainbow',\n",
        "              extent=[0., 50000., 0,  80000.],\n",
        "              origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"4%\", pad=0.05)\n",
        "fig.colorbar(h, cax=cax)\n",
        "\n",
        "ax.set_xlabel('$x$', fontsize = 15)\n",
        "ax.set_ylabel('$y\\ $', fontsize = 15, rotation = 0)\n",
        "ax.set_title('Inferred viscosity $\\mu(x,y)$', fontsize = 15)\n",
        "\n"
      ],
      "metadata": {
        "id": "DOLQMeRkx4gR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}